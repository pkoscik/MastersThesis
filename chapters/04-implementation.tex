
\chapter{Implementation}

\section{Implementing the cache model}

The entirety of the cache model has been implemented in the Python programming language. The motivation for this choice includes the portability of the code, ease of customization and
the ability to rapidly iterate and test different configurations.

\subsection{Model architecture}

\begin{center}
	\centering
	\includegraphics[width=\textwidth]{figures/04-implementation/cache_mdl_arch.pdf}
	\captionof{figure}{Visual representation of the cache model architecture}
	\label{fig:cache_mdl_arch}
\end{center}

\subsubsection{\texttt{Cache}} \label{sec:cache_model}

The Cache class encapsulates the cache memory behavioral model. Besides statistic gathering and helper debug methods, its only public interfaces are methods
providing read/write operations, cache flushing, and constructor with configuration parameters. The cache configuration is derived from the following inputs:
\begin{itemize}
	\item Cache width: width of cache memory - $\log_2(\text{cache size})$
	\item Block width: width of cache block - $\log_2(\text{block size})$
	\item Memory width: memory address width
	\item Lines per set: Set associativity: $2^n,\, n \in \{1, 2, 3, ...\}$; -1 for fully associative, 1 for direct mapping
	\item Replacement policy: FIFO, LRU, LFU or None for random
\end{itemize}

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:cache_ctor},
    caption={\texttt{Cache} constructor with configuration parameters}
    ]
def __init__(
	self,
	name: str,
	cache_width: int,
	block_width: int,
	memory_width: int,
	lines_per_set: int,
	replacement_policy: str
):
'''
    +----------------------------+--------------------+--------------+
    |            TAG                |           SET         |     OFFSET    |
    +----------------------------+--------------------+--------------+
    C                               B                       A               0

    TAG    = [C   : B]
    SET    = [B-1 : A]
    OFFSET = [A-1 : 0]
'''
# Width of the memories
self._cache_width = cache_width
self._block_width = block_width    # [A:0] width
self._memory_width = memory_width  # [C:0] width

# Convert __width of the bus__ to __size in bytes__
self._cache_size = 2 ** self._cache_width
self._block_size = 2 ** self._block_width
self._memory_size = 2 ** self._memory_width

self._num_lines = self._cache_size // self._block_size
self._lines = [CacheLine() for i in range(self._num_lines)]

if lines_per_set == -1:
    # special configuration case for fully associative mapping
    lines_per_set = self._num_lines

if not (lines_per_set & (lines_per_set - 1) == 0) or lines_per_set == 0:
    raise Exception('Lines per set must be a power of two (1, 2, 4, 8, ...)')

self._lines_per_set = lines_per_set
self._sets = self._num_lines // lines_per_set
self._set_width = int(math.log(self._sets, 2))  # calculate [B:A] width

self._replacement_policy = replacement_policy

# Statistics
self.misses = 0
self.hits = 0
self.invalidations = 0
self.flushes = 0
\end{lstlisting}
\end{minipage}
\end{center}

\subsubsection*{Read and write cache operations}
\noindent The \texttt{read} and \texttt{write} methods are the primary interfaces for the cache and have been implemented as follows:

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:cache_write_read},
    caption={\texttt{Cache} write and read interfaces}
    ]
def read(self, addr: int) -> None:
    self.printd(f'reading from cache @ {bin(addr)} (set {self._addr_get_set(addr)})')
    line, _, _ = self._line_lookup(addr)

    if line and not line.free:
        self.printd('[read] rhit')
        self.hits += 1
        line.use_count += 1
    else:
        self.printd('[read] rmiss')
        self.misses += 1
        line = self._load(addr)

def write(self, addr: int) -> None:
    self.printd(f'[write] writing to cache @ {hex(addr)}')
    line, _, _ = self._line_lookup(addr)

    if line:
        self.printd('[write] whit')
        self.hits += 1
    else:
        self.printd('[write] wmiss')
        self.misses += 1
        line = self._load(addr)
\end{lstlisting}
\end{minipage}
\end{center}

\noindent The code responsible for finding matching cache line for the given address has been extracted to the internal \footnote{The Python programming language does not have a
built-in mechanism for enforcing access rights to fields in a class. However, PEP 8 specifies that variables starting with '\texttt{\_}' should be used internally in the class
\cite{pep8}.} \texttt{\_line\_lookup} method:

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:line_lookup},
    caption={\texttt{Cache} line lookup}
    ]
def _line_lookup(self, addr: int) -> tuple[CacheLine, list, int] | None:
    tag = self._addr_get_tag(addr)
    set_index = self._addr_get_set(addr)
    lines_in_set = self._get_lines_for_set(set_index)

    line = None
    for line_candidate in lines_in_set:
       if line_candidate.tag == tag:
            line = line_candidate
            break

    return line, lines_in_set, tag if line else None
\end{lstlisting}
\end{minipage}
\end{center}

\noindent If a matching line is found, the function returns a reference to this line, other lines in the same set and the line tag. If no match can be made, the method returns \texttt{None}. This
method uses helper functions to extract the bit-fields from the memory address:


\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:bitfield_hepers},
    caption={Bitfields helper functions}
    ]
def create_mask(start_bit: int, end_bit: int) -> int:
    num_bits = end_bit - start_bit + 1
    mask = (1 << num_bits) - 1
    mask <<= start_bit
    return mask

def extract_bits(value: int, start_bit: int, end_bit: int) -> int:
    mask = create_mask(start_bit, end_bit)
    extracted_bits = (value & mask) >> start_bit
    return extracted_bits

class Cache:
    def _addr_get_tag(self, addr: int) -> int:
        ''' Extract [C:B] from the address '''
        start = self._block_width + self._set_width
        end = self._memory_width
        return extract_bits(addr, start, end)
    
    def _addr_get_set(self, addr: int) -> int:
        ''' Extract [B-1:A] from the address '''
        start = self._block_width
        end = self._block_width + self._set_width - 1
        return extract_bits(addr, start, end)
    
    def _addr_get_offset(self, addr: int) -> int:
        ''' Extract [A-1:0] from the address '''
        start = 0
        end = self._block_width - 1
        return extract_bits(addr, start, end)
\end{lstlisting}
\end{minipage}
\end{center}

\subsubsection*{Cache line loading}
\noindent The line loading is facilitated by the \texttt{\_load} method:

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:cache_load},
    caption={\texttt{Cache} load method}
    ]
def _load(self, phys_addr) -> CacheLine:
    self.printd(f'[load] loading @ {hex(phys_addr)} to cache')
    tag = self._phys_addr_get_tag(phys_addr)
    set_index = self._phys_addr_get_set(phys_addr)
    lines_in_set = self._get_lines_for_set(set_index)

    free_line_index = next((index for index, obj in enumerate(lines_in_set) if obj.free), None)
    if free_line_index is not None:
        index = free_line_index
    else:
        index = self._select_evicted_index(lines_in_set)
        self.invalidations += 1
        self.printd(f'[load] set: {set_index}, invalidating line: {index}')

	self.printd(f'[load] loading a new line to set: {set_index} with index: {index}')
    victim = lines_in_set[index]
    victim.free = False
    victim.tag = tag
    return victim
\end{lstlisting}
\end{minipage}
\end{center}

\subsubsection*{Cache flushing}
\noindent Cache flushing is implemented by re-initializing the internal cache state:

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:cache_flush},
    caption={\texttt{Cache} flush method}
    ]
def flush(self) -> None:
    self.printd('[flush] flushing cache')
    self.flushes += 1
    self._lines = [CacheLine() for i in range(self._num_lines)]
\end{lstlisting}
\end{minipage}
\end{center}

\subsubsection*{Debugging features}
\noindent The cache class also comes with a set of debugging helpers:
\begin{itemize}
    \item \texttt{print\_addr\_info}: prints address-related information in the specified format (binary or hexadecimal). It displays the address, tag, set, and offset values.
    \item \texttt{print\_cache\_info}: prints details about the cache configuration, including the width of the tag, set, and block fields, as well as the size and number of various cache components.
    \item \texttt{print\_hmr}: prints the hit-miss ratio along with the number of hits, misses, and invalidations.
    \item \texttt{print\_debug\_lines}: prints debug information for each cache line, including the tag, whether the line is free, and its use count.
\end{itemize}

\subsubsection{\texttt{CacheLine}}

The \texttt{CacheLine} class represents a cache line entry:

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:cacheline},
    caption={\texttt{CacheLine} class}
    ]
class CacheLine:
    def __init__(self):
        self.tag = 0b0
        self.use_count = 0x0
        self.free = True
\end{lstlisting}
\end{minipage}
\end{center}

\noindent
\begin{itemize}
    \item \textbf{Tag:} Binary identifier distinguishing the data block, extracted from the physical address.
    \item \textbf{Use Count:} Counter for replacement policies; tracks accesses (LFU), recency (LRU), or order (FIFO).
    \item \textbf{Free:} Boolean flag; \texttt{True} if the line is free for new data, \texttt{False} if it contains valid data, and will need to be evicted.
\end{itemize}


% TODO: these should be moved somewhere else, as we need renode traces - or I could just forward-reference or sth
\section{Cache interfaces}

The \textit{cache model} (\ref{sec:cache_model}) does not implement any execution, log parsing, and validation logic. This is a deliberate choice to maintain high reusability and modifiability
of the emulation core component. To test and use the cache model, two interfaces were implemented:

\begin{itemize}
	\item \textbf{DummyLogInterface:} a "dummy" interface, intended for usage in test cases. Does not implement any log parsing, instead, the memory accesses are passed as a list of
        memory operations. Does not provide any user configurable cache parameters options.
    \item \textbf{RenodeLogInterface:} Interface intended to be used with Renode's \textit{ExecutionTracer} log format. Implements a command-line interface, allowing for easy cache
        configuration changes, as well as a set of preconfigured cache configurations for a selection of devices.
\end{itemize}


\subsection{\texttt{DummyLogInterface}}

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:dummyinterface},
    caption={\texttt{DummyLogInterface} implementation}
    ]
class DummyLogInterface:
    def __init__(self, cache: Cache):
        self.cache = cache
        self.count_insn_read = 0
        self.count_mem_read = 0
        self.count_mem_write = 0
        self.count_io_read = 0
        self.count_io_write = 0

    def simulate(self, data: dict):
        for access in data:
            for type, addr in access.items():
                match type:
                    case 'mw':
                        self.count_mem_write += 1
                        self.cache.write(addr)
                    case 'mr':
                        self.count_mem_read += 1
                        self.cache.read(addr)
                    case 'ior':
                        self.count_io_read += 1
                    case 'iow':
                        self.cache.flush()
                        self.count_io_write += 1
                    case _:
                        raise ValueError('Unsupported memory operation!')
\end{lstlisting}
\end{minipage}
\end{center}

\noindent Example usage:

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:dummyinterface_usage},
    caption={\texttt{DummyLogInterface} usage}
    ]
cache = Cache(
    name='cache',
    cache_width=6,
    block_width=2,
    memory_width=10,
    lines_per_set=4,
    replacement_policy=None,
)
cinterface = DummyLogInterface(cache)
ops = [
    {'mr': 0b000000_00_00},
    {'mw': 0b000000_00_00},
    {'mr': 0b000000_10_00},
    {'ior': 0b000000_11_00},    
]
cinterface.simulate(ops)
\end{lstlisting}
\end{minipage}
\end{center}

\noindent The main goal of this interface is to wrap the cache model, allowing it to be mainly used later to create and run test scenarios, find and remove implementation errors, and
serve as a test bench for developing new functionalities. It also serves as a generic implementation, that can derived from, and used to aid in connecting this cache model simulator
to other simulators. On its own this model does not, neither it aims to, provide any external trace interface.

\subsection{\texttt{RenodeLogInterface}}

The \texttt{RenodeLogInterface} is an interface allowing for usage of the cache model with the execution trace files generated from the Renode Framework \textit{ExecutionTracer} logging module.
This functionality can be enabled by adding these lines to the platform script file\footnote{Creating, configuring and working with Renode virtual platforms is described in the
(\ref{app:creating_renode_platforms}) section.}:

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    label={lst:renode_enabling_exectracer},
    caption={Enabling \texttt{ExeuctionTracer}\protect\footnotemark}
    ]
<cpu_name> MaximumBlockSize 1
<cpu_name> CreateExecutionTracing "tracer" $ORIGIN/trace.log PCAndOpcode
tracer TrackMemoryAccesses
\end{lstlisting}
\end{minipage}
\end{center}
\footnotetext{The \texttt{<cpu\_name>} must be changed to the name of the CPU on which the execution tracer should be activated.}

\noindent Running a Renode simulation with the tracer enabled will generate a \texttt{trace.log} file:
\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    label={lst:exec_tracer_example},
    caption={Example ExecutionTracer output}
    ]
0x8002110: 0x00B78633
0x8002114: 0x4210
MemoryRead with address 0x8046FC4
0x8002116: 0x973E
0x8002118: 0x0791
0x800211A: 0xC310
MemoryIOWrite with address 0x100B02AC
0x800211C: 0xFED799E3
0x800210E: 0x6018
MemoryRead with address 0x8046CF0
0x8002110: 0x00B78633
0x8002114: 0x4210
MemoryRead with address 0x8046FC8
\end{lstlisting}
\end{minipage}
\end{center}

\noindent The \texttt{RenodeLogInterface} processes these execution trace logs, extracting memory access events for the cache simulation. In the following section, we will review % TODO: improve this
the source code for the \texttt{RenodeLogInterface}, focusing on reading trace files, interfacing with the cache model and streamlining cache parameters configuration.

\subsubsection*{Cache memory configuration}

In order to streamline the cache configuration, this application allows for two ways of ways of cache model configuration:
\begin{itemize}
    \item \textbf{Command line interface parameters:} this method allows users to specify detailed configuration options directly through the command line. Users can define various
        cache parameters such as memory width, cache width, block width, lines per set, and the replacement policy for both L1 instruction and data caches.
    \item \textbf{Presets:} this method loads a predefined set of configuration parameters using a preset name. Presets are useful for standard configurations that are frequently used or
        for quickly setting up the cache model without specifying each parameter individually.
\end{itemize}

\noindent The presets are defined in the interface source code, and are described using the following format:
\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    caption={Cache configuration presets\protect\footnotemark}
    ]
presets = {
    'fu740.u74': {
        'l1i': Cache('l1i,u74', 15, 6, 64, 4, None),
        'l1d': Cache('l1d,u74', 15, 6, 64, 8, None),
        'flush_opcodes': {
            'i': [0xfc100073],
            'd': [0xfc000073]
        }
    },
    'fe310.e31': {
        'l1i': Cache('l1i,e31', 14, 5, 32, 2, None)
    }
}
\end{lstlisting}
\end{minipage}
\end{center}
\footnotetext{Parameters that are not available on a given platform can be skipped and not included in the preset.} % XXX: drop this?

\noindent Usage using build in presets: \texttt{./renode\_cache\_mdl.py trace.log presets 'fu740.u74'}, equivalent command line parameters:

\begin{verbatim}
./renode_cache_mdl.py trace.log config    \
    --memory_width 64                     \
    --i_invalidation_opcodes '0xfc100073' \
    --d_invalidation_opcodes '0xfc000073' \
    --l1i_cache_width 15                  \
    --l1i_block_width 6                   \
    --l1i_lines_per_set 4                 \
    --l1d_cache_width 15                  \
    --l1d_block_width 6                   \
    --l1d_lines_per_set 8
\end{verbatim}

\subsubsection*{Parsing \texttt{ExecutionTracer} logs}

The traces generated by the \textit{ExecutionTracer} can reach up to several gigabytes in size\footnote{The \textit{ExecutionTracer} also supports saving traces in a custom binary format. However,
this approach was not used because it would add an additional layer of complexity and reduce the readability of the code.}, which is why it is necessary to parse them line by line. 
After each line is parsed, the cache model state is updated accordingly.

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:renodeinterface},
    caption={\texttt{RenodeLogInterface} simulate implementation}
    ]
def simulate(self):
    """ Simulate the cache structure

    Due to _large_ trace files, parse the file line-by-line.

    Renode ExecutionTracer outputs the following data:

    * `PC`: `OPCODE`
    * Memory{Write, Read} with address `ADDR`
    * MemoryIO{Write, Read} with address `ADDR`
    """
    with open(self.fname, 'r') as f:
        for line in tqdm(f, total=lines):
            if ':' in line and self.l1i is not None:
                self.count_insn_read += 1
                pc, opcode = (int(value.strip(), 16) for value in line.split(":"))
                if opcode in self.invalidation_opcodes:
                    match self.invalidation_opcodes[opcode]:
                        case 'i':
                            self.l1i.flush()
                        case 'd':
                            self.l1d.flush()
                self.l1i.read(pc)
            elif line.startswith('Memory') and self.l1d is not None:
                parts = line.split()
                address = int(parts[-1], 16)
                match parts[0].lower().removeprefix('memory'):
                    case 'iowrite':
                        self._handle_io(self, 'write')
                        self.count_io_write += 1
                    case 'ioread':
                        self._handle_io(self, 'read')
                        self.count_io_read += 1
                    case 'write':
                        self.count_mem_write += 1
                        self.l1d.write(address)
                    case 'read':
                        self.count_mem_read += 1
                        self.l1d.read(address)
                    case _:
                        raise ValueError('Unsupported memory operation!')
\end{lstlisting}
\end{minipage}
\end{center}

\section{Integrated test bench}
The integrated test bench is designed to evaluate the correctness of the cache model through three independent test cases, each verifying a different cache placement method:
set associative, fully associative, and direct mapping. In each case, the cache was configured with a 1 KiB size, 4 byte block size, and 10-bit memory addressing\footnote{This
configuration was arbitrarily chosen with the goal of operating on a small number of cache lines, making it easier to work with as a test example.}, with the only difference
being the mapping method. After the configuration, each test performs a set of memory operations that verify the proper behavior when reading from both valid and empty lines, line
eviction behavior, flushing, and correct set/index matching. A snippet of the test for the set associative cache has been included below:

\begin{center}
\centering
\begin{minipage}{\linewidth}
\begin{lstlisting}[
    language=Python,
	morekeywords={self},
    label={lst:cache_write_read},
    caption={Snippet of integrated testbench - set associative unit test}
    ]
cache = Cache(
    name='set_associative',
    cache_width=6,
    block_width=2,
    memory_width=10,
    lines_per_set=4,
    replacement_policy='FIFO',
)
test = DummyLogInterface()
test.configure_caches(cache)

test.simulate([
    {'mr': 0b000000_00_00},  # Set 0
    {'mr': 0b000000_01_00},  # Set 1
    {'mr': 0b000000_10_00},  # Set 2
    {'mr': 0b000000_11_00},  # Set 3
])
assert test.cache.hits == 0
assert test.cache.misses == 4
assert test.cache.invalidations == 0

test.simulate([
    {'mr': 0b000000_00_11},  # Set 0
    {'mr': 0b000000_01_11},  # Set 1
    {'mr': 0b000000_10_11},  # Set 2
    {'mr': 0b000000_11_11},  # Set 3
])
assert test.cache.hits == 4
assert test.cache.misses == 4
assert test.cache.invalidations == 0

...
\end{lstlisting}
\end{minipage}
\end{center}

