
\chapter{Background}


\section{System emulation}

% TODO: add footnote explaining that emulation and simulation will be used interchangebly
% TODO: please for the love of GOD refactor/proofread this section
System emulation is a technique of modeling and imitating the hardware of one
system on another system. The range of emulation depends on the use case; some
emulators, such as Wine or QEMU in KVM mode, only emulate system-level calls,
executing the rest of the application natively. Another example of emulation
is software like VirtualBox, VMWare, or Microsoft Hyper-V, which uses host
processor virtualization extensions to run the guest operating system in a
supervised environment, allowing for more separation between the guest software.
More advanced emulators, such as the Renode framework, can emulate the entire
platform, including processors and peripherals of another architecture. Another
notable example of such an emulator is QEMU's system mode emulation and Intel
Simics.

Emulation software has become a crucial part of developing modern-day software
and hardware. It finds uses in a variety of fields of software world, starting
from enabling cloud services providers with secure, reproducible, and isolated
execution environments. This task is mainly handled by the level one and
two hypervisors such as VMWare or Hyper-V. Another use for emulators is the
usage in continuous integration and delivery systems, where Kernel Virtual
Machines (KVM), such as QEMU KVM, are widely used to aid in deploying so-called
"runners", that execute testing and deployment software in a manageable and
scalable manner. Yet another scenario where emulation greatly enhances workflow
is the development of embedded/edge devices. Emulation software allows for
the development of software for non-PC platforms - such as microcontrollers,
SBCs or FPGA devices - without the need for physical hardware. This decoupling
has become important as development teams have grown larger and larger. This
streamlines the embedded software development process by further aligning it
with classical software development practices. Emulation solutions provide
first-class features such as reproducible CI pipelines, code coverage results
and other tools that have been taken for granted in the desktop development.

%

\section{Caches and memory hierarchies}

% TODO: some bullshit about how computers work here

\subsection{Overview of memory hierarchy}
\subsection{Role of caches in system performance}
%

\section{CPU caches}
%
\subsection{Placement policies}
Cache placement policies determine where a specific memory block can be loaded into
the cache. The choice of placement policy influences the cache architecture and
its control logic - affecting the overall complexity and performance of the system.
Each policy involves trade-offs between speed, by the means of reducing cache misses and
thrashing, and hardware costs related to the size and design of the hardware.

\vspace{10px}
\noindent Memory and cache configuration for all examples in this section:
\begin{itemize}
	\item Memory size: 1Â KiB
	\item Cache size: 32 bytes
	\item Cache block size: 4 bytes
	\item Number of lines: 8
\end{itemize}

\subsubsection{Fully associative cache}
\begin{center}
	\centering
	\includegraphics[width=\textwidth]{figures/02-background/full_ass_mem.pdf}
	\captionof{figure}{Visual representation of fully associative mapping}
	\label{fig:full_ass_mem}
\end{center}

\noindent In the fully associative cache each \textit{cache line} can hold a copy of \textit{any memory location}. The memory address is split into the following bit fields:

\begin{itemize}
	\item \textbf{Offset:} bits used to determine the byte to be accessed from the cache line. In this example there are two bits, that are used to address 4 bytes in the cache line.
	\item \textbf{Tag:} is used by the hardware comparator in the cache hardware design to match the address on cache request. It takes up the rest of the address, in our example 8 bits.
\end{itemize}


\noindent This configuration minimizes the chances of cache misses due to conflicts, potentially improving performance. However, this type of cache requires complex hardware for
searching and managing, as it needs to check all entries simultaneously - this means that there are needed as many comparators as there are bits in the \textit{tag} field.
This means that such implementation is unpractical and seldom used for larger caches \cite{whatevery}, this results in high power consumption and greatly increased die size.

\subsubsection{Set associative cache}
\begin{center}
	\centering
	\includegraphics[width=\textwidth]{figures/02-background/set_ass_mem.pdf}
	\captionof{figure}{Visual representation of 4-way set associative mapping}
	\label{fig:set_ass_mem}
\end{center}

\noindent The set associative cache introduces a concept of a \textit{set} - a collection of more than one cache line sharing the same \textit{tag}.

\begin{itemize}
	\item \textbf{Offset:} bits used to determine the byte to be accessed from the cache line. In this example there are two bits, that are used to address 4 bytes in the cache line.
	\item \textbf{Set:} (sometimes also called \textbf{index}) bits are used to select the cache set comparators. This example presents 4-way set associative cache - meaning that there are 4 lines
		in each set - needing 2 bits to address it.
	\item \textbf{Tag:} is used by the hardware comparator in the cache hardware design to match the address on cache request. It takes up the rest of the address, in our example 6 bits.
\end{itemize}

\noindent This configuration exchanges the flexibility of fully associative placement, for a significantly simplified hardware model. The reduced flexibility comes from the fact that
not every memory address can be cached at the same time, let's imagine that the following addresses are loaded to the cache \texttt{0b0000\_00\_00}, \texttt{0b0001\_00\_00}, \texttt{0b0011\_00\_00}
and \texttt{0b0111\_00\_00}; all of these four lines share the same \textit{set} \texttt{0b00}, loading another address with the same set, for example \texttt{0b1000\_00\_00} would require 
an eviction of one of those lines. Eviction methods will be covered in the (\ref{sec:eviction_policies}) section.

% \noindent This is the most popular type of cache \cite{digitaldesgnandcomp}.

\subsubsection{Directly mapped cache}
In the directly mapped cache each \textit{cache line} can hold a copy of a single \textit{tag}. This policy is equivalent to the set associative cache, with 1-way associativity.
% TODO: image, diagram

%
\subsection{Replacement policies} \label{sec:eviction_policies}
In cache design, the replacement policies describe behavior that is performed when a line needs to get \textit{evicted} from the cache memory.
The choice of replacement policy is a trade-off between cache performance and design complexity required to implement given solution. Low power
devices, such as embedded microcontrollers often opt of simple implementations, such as \textit{random eviction}, or \textit{FIFO}, as these require % TODO: add \ref to these methods
a lower amount of digital logic to select a line to evict, decreasing power consumption. High performance processors, found in PCs, servers, tablets and smartphones,
often use more advanced policies, such as \textit{LRU} and \textit{LFU}.

\subsubsection{Queue based}
% TODO(WHOLE PAR): replace 'policy' - used to often
Queue-based replacement policies use data structures such as queues to manage the replacement process. The most common queue based policy is the
\textit{First In First Out (FIFO)} policy. In this implementation the oldest cache line is replaced first. The main advantages of this policy are  % TODO: replace 'implementation'
its relative ease of implementation and predictable eviction behavior. That simplicity comes at a cost, as this policy does not guarantee optimal cache usage. The queue
based policies do not consider the frequency or recency of the data access when evicting lines, this can lead to a scenario where a data line, that has just been accessed - and due to
temporal locality - is likely to be accessed again, might get prematurely evicted from the cache, simply because it was the first loaded line.

\vspace{10px}\noindent Other queue based replacement policies include: Circular Buffer, Second Chance or Clock algorithms. % TODO: find citations

\subsubsection{Recency based}
Recency based policies prioritize data that has been accessed most recently - leveraging the concept of temporal locality - addressing one of
the main drawback of queue based policies. One of the examples of such policy is \textit{Least Recently Used (LRU)} policy. In this approach, the cache controller
keeps track of the recency of data by maintaining an ordered list of cache lines, where the most recently accessed lines are moved to the front. Due to the need of monitoring
the usage data, LRU (or other frequency based policies) require much more complex hardware designs, increasing power consumption. 

\vspace{10px}\noindent Other recency based replacement policies include: Most Recently Used, LFU with Aging or LRU-K. % TODO: find citations

\subsubsection{Frequency based}
Frequency based policies attempt to leverage the temporal locality of the memory transfers, by prioritizing the data that has been used (read) most frequently.
An example of these policies is the \textit{Least Frequently Used (LFU)}. Similarly to the recency based approaches, the cache controller tracks additional usage counters
for each line, related to the amount of times that the address has been accessed - this increases performance at the cost of increased complexity.

\vspace{10px}\noindent Other frequency based replacement policies include: Adaptive Replacement Cache, Two-Level Adaptive Replacement Cache or Least Frequently Used with Dynamic Aging. % TODO: find citations
%
\subsection{Cache coherency}

The goal of cache coherency is to maintain a consistent state between two or
more separate cache memories. This process can take place both in single-core
systems - in which the state is synchronized between two or more different levels of caches
- and in multicore processors, where the caches are kept in sync between multiple
processors.

\subsubsection{Direct memory access} \label{sec:dma}
% TODO: citations
Direct memory access, also referred to as \textit{DMA}, is a mechanism that
frees up the CPU cycles dedicated to memory operations. It is commonly used 
in one of two configurations:

\begin{itemize}
	\item \textbf{I/O to memory DMA:} In this mode, the CPU initiates the data block % TODO: back this up
		transfer, after which the DMA controller takes over to move or copy the data.
		While the transfer is ongoing, the CPU remains free to perform other operations.
		The processor receives an interrupt from the DMA controller once the transfer is complete.
		This arrangement allows for multitasking during data transactions.
	\item \textbf{Memory to memory DMA:} In this configuration the peripheral can perform memory % TODO: this too
		transfers completely independently of the CPU. This allows for high speed data transfers
		between different memory regions - often spanning multiple devices - without increasing
		the processor workload.
\end{itemize}

\noindent DMA functionality can be found in a variety of modern devices and
peripherals, starting from the relatively simple implementations found in the
UART and USART controllers, passing through more complex setups in network % TODO: cite some examples of DMA here
interfaces, and ending on high-performance computation accelerators (GPU/NPU)
and storage controllers.

\noindent In the cache coherency context, special care needs to be % TODO: cite something
taken when utilizing and implementing DMA mechanisms, the figure
(\ref{fig:dma_cache_issues}) presents the example of memory $\leftrightarrow$
cache consistency error caused by an DMA access.

\begin{center}
	\centering
	\includegraphics[width=\textwidth]{figures/02-background/dma_cache_issues.pdf}
	\captionof{figure}{Coherency error caused by a DMA transfer}
	\label{fig:dma_cache_issues}
\end{center}

\noindent This example starts off with the main memory and first level data cache - referred to as \texttt{l1d\$}, in sync.
A DMA operation updates the main memory address \texttt{0x8000\_0000} to a new value \texttt{0xFF} - The change is performed directly
in the memory, bypassing the CPU cache system. When CPU accesses this memory address, it retrieves the value from its
cache line, which has not been updated, as the DMA transfer was completely transparent for the processor.
In system design, there are two main ways to address such problems:

\begin{itemize}
	\item \textbf{Hardware approach:} \textit{bus snooping} (sometimes also referred to as\textit{bus sniffing}) is an % TODO: cite bus spoofing
		approach where an additional hardware coherency controller - called \textit{snooper} - monitors the outstanding
		transfers on the bus. When external access to a bus is detected, it sends a signal to the cache system, where the
		effected caches are either flushed, or the invalid lines are marked as \textit{dirty}. The primary advantage of this solution is the reduced
		complexity of the operating system and/or drivers, coupled with the CPU executing fewer instructions - at the cost of increased hardware complexity. % TODO: lots of 'complexity' here, refactor so it flows better
	\item \textbf{Software approach:} in designs where implementing hardware coherency solution is not possible - due to space, power or cost constraints -
		the software solutions must be used. In these cases the operating system, or the device drivers, must keep track of ongoing DMA transfers, and manually
		invoke cache invalidating and flushing instructions when necessary. While such solutions reduce the overall hardware design complexity, they add a
		runtime overhead for the processor.
\end{itemize}


\subsubsection{Symmetric multiprocessing}
% TODO: citations

In the context of CPU's, symmetric multiprocessing (or shared-memory multiprocessing) is a system architecture, where two or more \textbf{identical processors} % TODO: add a footnote that SMP implies homogeneous systems (and CITE this as this is a pretty bold statement lol)
are running \textbf{independently of each other}. In SMP systems processors share a common memory pool, allowing for data sharing and communication
between cores. Such configuration requires that multiple cores share a common bus. Some examples of such buses include \textit{Front-side bus}, \textit{Intel Ultra Path Interconnect} % TODO: add citations here (yes, I'm artifically inflating the bilbiography :))
and \textit{HyperTransport} on the x86 personal computers; examples used in embedded systems are \textit{Arm Advanced Microcontroller Bus Architecture} and \textit{SiFive TileLink}. % TODO: citation for AMBA TODO: XXX: is tilelink valid here?

One of the key benefits of SMP designs is their ability to increase performance in a scalable manner, benefiting from parallelization of workloads among multiple cores. % TODO: really please start citing this
Another advantage of such designs is the increased data throughput between multiple cores, which tend to be physically located close to each other on the silicon die.
This proximity allows for much higher bus clock frequencies, thereby increasing the maximum possible data transfer bandwidth on the shared bus. % TODO: lots of 'bus' here

In the context of cache coherency, processors in SMP systems typically share not only the main memory but also higher-level caches - such as level 2 and level 3 caches.
The contents of first level of cache are usually not shared between cores. Despite L1 being private to each core, coherence must be maintained across all cache levels. % TODO: footnote explaining cache sync vs cache cocherence
This is achieved by using various \textit{coherence protocols}, such as \textbf{MESI} (Modified, Exclusive, Shared, Invalid) and \textbf{MOESI} (Modified, Owned, Exclusive, Shared, Invalid).
This work will not cover the specific implementation details of these protocols. % XXX: nice cliffhanger ;) but rly maybe I should at least to try to put a simplified diagram 'ere?

