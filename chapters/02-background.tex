
\chapter{Background}


\section{System emulation}

% TODO: add footnote explaining that emulation and simulation will be used interchangebly
% TODO: please for the love of GOD refactor/proofread this section
System emulation is a technique of modeling and imitating the hardware of one
system on another system. The range of emulation depends on the use case; some
emulators, such as Wine or QEMU in KVM mode, only emulate system-level calls,
executing the rest of the application natively. Another example of emulation
is software like VirtualBox, VMWare, or Microsoft Hyper-V, which uses host
processor virtualization extensions to run the guest operating system in a
supervised environment, allowing for more separation between the guest software.
More advanced emulators, such as the Renode framework, can emulate the entire
platform, including processors and peripherals of another architecture. Another
notable example of such an emulator is QEMU's system mode emulation and Intel
Simics.

Emulation software has become a crucial part of developing modern-day software
and hardware. It finds uses in a variety of fields of software world, starting
from enabling cloud services providers with secure, reproducible, and isolated
execution environments. This task is mainly handled by the level one and
two hypervisors such as VMWare or Hyper-V. Another use for emulators is the
usage in continuous integration and delivery systems, where Kernel Virtual
Machines (KVM), such as QEMU KVM, are widely used to aid in deploying so-called
"runners", that execute testing and deployment software in a manageable and
scalable manner. Yet another scenario where emulation greatly enhances workflow
is the development of embedded/edge devices. Emulation software allows for
the development of software for non-PC platforms - such as microcontrollers,
SBCs or FPGA devices - without the need for physical hardware. This decoupling
has become important as development teams have grown larger and larger. This
streamlines the embedded software development process by further aligning it
with classical software development practices. Emulation solutions provide
first-class features such as reproducible CI pipelines, code coverage results
and other tools that have been taken for granted in the desktop development.

%

\section{Caches and memory hierarchies}

% TODO: some bullshit about how computers work here

\subsection{Overview of memory hierarchy}
\subsection{Role of caches in system performance}
%

\section{CPU caches}
\subsection{Basic configuration parameters}
%
\subsection{Placement policies}
Cache placement policies determine where a specific memory block can be loaded into
the cache. The choice of placement policy influences the cache architecture and
its control logic - affecting the overall complexity and performance of the system.
Each policy involves trade-offs between speed, by the means of reducing cache misses and
thrashing, and hardware costs related to the size and design of the hardware.

% TODO: explain how cache is adressed, how entry is split etc

\subsubsection{Fully associative cache}
In the fully associative cache each \textit{cache line} can hold a copy of
\textit{any memory location}.
% TODO: image, diagram

\noindent This configuration minimizes the chances of cache misses due to conflicts,
potentially improving performance. However, this type of cache requires
complex hardware for searching and managing, as it needs to check all entries
simultaneously. This results in higher power consumption and increased die size.

\subsubsection{Set associative cache}
The set associative cache introduces a concept of a \textit{set} - a collection
of more than one cache line.
% TODO: image, diagram

\subsubsection{Directly mapped cache}
In the directly mapped cache each \textit{cache line} can hold a copy of
a single \textit{tag}. This policy is equivalent to the set associative cache, with
1-way associativity.
% TODO: image, diagram

%
\subsection{Replacement policies}
\subsubsection{Queue based}
\subsubsection{Recency based}
\subsubsection{Frequency based}
%
\subsection{Cache coherency}

The goal of cache coherency is to maintain a consistent state between two or
more separate cache memories. This process can take place both in single-core
systems - in which the state is synchronized between two or more different levels of caches
- and in multicore processors, where the caches are kept in sync between multiple
processors.

\pagebreak
\subsubsection{Direct memory access}
% TODO: citations
Direct memory access, also referred to as \textit{DMA}, is a mechanism that
frees up the CPU cycles dedicated to memory operations. It is commonly used 
in one of two configurations:

\begin{itemize}
	\item \textbf{I/O to memory DMA:} In this mode, the CPU initiates the data block % TODO: back this up
		transfer, after which the DMA controller takes over to move or copy the data.
		While the transfer is ongoing, the CPU remains free to perform other operations.
		The processor receives an interrupt from the DMA controller once the transfer is complete.
		This arrangement allows for multitasking during data transactions.
	\item \textbf{Memory to memory DMA:} In this configuration the peripheral can perform memory % TODO: this too
		transfers completely independently of the CPU. This allows for high speed data transfers
		between different memory regions - often spanning multiple devices - without increasing
		the processor workload.
\end{itemize}

\noindent DMA functionality can be found in a variety of modern devices and
peripherals, starting from the relatively simple implementations found in the
UART and USART controllers, passing through more complex setups in network % TODO: cite some examples of DMA here
interfaces, and ending on high-performance computation accelerators (GPU/NPU)
and storage controllers.

\noindent In the cache coherency context, special care needs to be % TODO: cite something
taken when utilizing and implementing DMA mechanisms, the figure
(\ref{fig:dma_cache_issues}) presents the example of memory $\leftrightarrow$
cache consistency error caused by an DMA access.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{figures/02-background/dma_cache_issues.pdf}
	\caption{Coherency error caused by a DMA transfer}
	\label{fig:dma_cache_issues}
\end{figure}

\noindent This example starts off with the main memory and first level data cache - referred to as \texttt{l1d\$}, in sync.
A DMA operation updates the main memory address \texttt{0x8000\_0000} to a new value \texttt{0xFF} - The change is performed directly
in the memory, bypassing the CPU cache system. When CPU accesses this memory address, it retrieves the value from its
cache line, which has not been updated, as the DMA transfer was completely transparent for the processor.
In system design, there are two main ways to address such problems:

\begin{itemize}
	\item \textbf{Hardware approach:} \textit{bus snooping} (sometimes also referred to as\textit{bus sniffing}) is an % TODO: cite bus spoofing
		approach where an additional hardware coherency controller - called \textit{snooper} - monitors the outstanding
		transfers on the bus. When external access to a bus is detected, it sends a signal to the cache system, where the
		effected caches are either flushed, or the invalid lines are marked as \textit{dirty}. The primary advantage of this solution is the reduced
		complexity of the operating system and/or drivers, coupled with the CPU executing fewer instructions - at the cost of increased hardware complexity. % TODO: lots of 'complexity' here, refactor so it flows better
	\item \textbf{Software approach:} in designs where implementing hardware coherency solution is not possible - due to space, power or cost constraints -
		the software solutions must be used. In these cases the operating system, or the device drivers, must keep track of ongoing DMA transfers, and manually
		invoke cache invalidating and flushing instructions when necessary. While such solutions reduce the overall hardware design complexity, they add a
		runtime overhead for the processor.
\end{itemize}


\subsubsection{Symmetric multiprocessing}
% TODO: citations

In the context of CPU's, symmetric multiprocessing (or shared-memory multiprocessing) is a system architecture, where two or more \textbf{identical processors} % TODO: add a footnote that SMP implies homogeneous systems (and CITE this as this is a pretty bold statement lol)
are running \textbf{independently of each other}. In SMP systems processors share a common memory pool, allowing for data sharing and communication
between cores. Such configuration requires that multiple cores share a common bus. Some examples of such buses include \textit{Front-side bus}, \textit{Intel Ultra Path Interconnect} % TODO: add citations here (yes, I'm artifically inflating the bilbiography :))
and \textit{HyperTransport} on the x86 personal computers; examples used in embedded systems are \textit{Arm Advanced Microcontroller Bus Architecture} and \textit{SiFive TileLink}. % TODO: citation for AMBA TODO: XXX: is tilelink valid here?

One of the key benefits of SMP designs is their ability to increase performance in a scalable manner, benefiting from parallelization of workloads among multiple cores. % TODO: really please start citing this
Another advantage of such designs is the increased data throughput between multiple cores, which tend to be physically located close to each other on the silicon die.
This proximity allows for much higher bus clock frequencies, thereby increasing the maximum possible data transfer bandwidth on the shared bus. % TODO: lots of 'bus' here
